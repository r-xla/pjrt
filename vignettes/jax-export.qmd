---
title: "Exporting JAX Models to R with StableHLO/XLA"
format: html
---

## Python: Export JAX Model and Weights

Using jax versin *0.4.35*, we do:

```{python}
#| eval: false
import jax
import jax.numpy as jnp
import numpy as np
from jax import export
import safetensors
from safetensors.numpy import save_file
import os

# 1. Define a simple neural network (no training)
def relu(x):
    return jnp.maximum(0, x)

@jax.jit
def forward(params, x):
    w1, b1, w2, b2 = params
    x = relu(jnp.dot(x, w1) + b1)
    x = jnp.dot(x, w2) + b2
    return x


# Randomly initialize weights
key = jax.random.PRNGKey(0)
w1 = jax.random.normal(key, (4, 8))
b1 = jax.random.normal(key, (8,))
w2 = jax.random.normal(key, (8, 2))
b2 = jax.random.normal(key, (2,))
params = [w1, b1, w2, b2]

# 2. Train your model

# ...

# 3. Export weights to safetensors
weights = {
    "w1": np.array(w1),
    "b1": np.array(b1),
    "w2": np.array(w2),
    "b2": np.array(b2),
}
weights_path = "weights.safetensors"
save_file(weights, weights_path)

# 4. Export the model to XLA (HLO)
exp = export.export(forward)(params, jnp.ones((1, 4)))

# ----- optional but typical persistency steps -----------------------------
# Human-readable MLIR (StableHLO dialect)
with open("jax-mlp.mlir", "w") as f:
    f.write(exp.mlir_module())               # textual StableHLO

# TODO: Maybe use binary format?
# Compact FlatBuffer for deployment or archival
#with open("model_stablehlo.bin", "wb") as f:
#    f.write(exp.serialize())                 # byte-array with metadata
```

TODO: Decide between MLIR and flatbuffer / or do both?

## R: Load Weights and Model, Run Inference

```{r}
library(safetensors)   # from ~/r-xla/safetensors
library(pjrt)          # from this package

# 1. Load weights from safetensors
weights <- safe_load_file("weights.safetensors", framework = "pjrt")

# 2. Load the model (StableHLO/XLA) using pjrt
model_src <- paste0(readLines("model_stablehlo.mlir"), collapse = "\n")
program <- pjrt_program(model_src, format = "mlir")
compiled <- pjrt_compile(program)

# 3. Create a new tensor and execute the model
input_tensor <- pjrt_buffer(matrix(1, nrow = 1, ncol = 4), type = "f32")
result <- pjrt_execute(compiled, weights$w1, weights$b1, weights$w2, weights$b2, input_tensor)
as_array(result)
```
